<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
        "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
    <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/"/>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
    <link rel="stylesheet" href="css/main.css" type="text/css"/>
    <script type="text/javascript" src="js/main.js"></script>
    <link rel="shortcut icon" href="img/favicon.png"/>
    <link rel="bookmark" href="img/favicon.png" type="image/x-icon" 　/>
    <title>Welcome to Shuo's HomePage</title>
</head>
<body>
<!--<div class="aaamenu">-->
<!--<td>-->
<!--<a href="#aboutme">About me</a>-->
<!--<a href="https://scholar.google.com/citations?user=HWwI12YAAAAJ&hl=en" style="margin-left: 10px">Google Scholar</a>-->
<!--<a href="https://github.com/SHUO-HUAI" style="margin-left: 10px">GitHub</a>-->
<!--</td>-->
<!--</div>-->
<table summary="Table for page layout." id="tlayout">
    <tr valign="top">
        <td id="layout-content">

            <div id="toptitle" style="position: fixed; background-image: url('./img/background.png');
             background-size: cover; background-repeat: no-repeat;">
                <div style="display: inline">
                    <h1 style="display: inline; ">Huai, Shuo (槐硕） </h1></div>
                <div class="aaamenu" style="display: inline"><a href="#aboutme">About me</a>
                    <a href="#publications" style="margin-left: 10px; ">Publications</a>
                    <a href="#projects" style="margin-left: 10px">Projects</a>
                    <a href="#experience" style="margin-left: 10px">Experience</a>
                    <a href="doc/Curriculum_Vitae.pdf" style="margin-left: 10px">CV</a>
                    <button onclick="PhoneOrDesktop()" style="margin-left: 10px;" class="btn">Blog</button>
<!--                    <a href="indexzh.html" style="margin-left: 10px">English/中文</a>-->
                </div>


            </div>
            <div style="position: fixed; top: 2.5em ;background: #ffffff; width: 100%;	z-index: 9998;">
                <br/> <br/> <br/> <br/>
            </div>

            <div style="position: absolute; margin-top: 6.8em; width: 97.5%">
                <table class="imgtable">
                    <tr>
                        <td>
                            <a href="img/bio.png"> <img src="img/bio.png" alt="alt text" width="131px" height="174px"/>&nbsp;&nbsp;&nbsp;
                            </a>
                        </td>
                        <td align="left"><p>Ph.D. Candidate<br/>
                            School of Computer Science and Engineering,<br/>
                            Nanyang Technological University. <br/>
                            50 Nanyang Ave, 639798, Singapore. <br/><br/>
                            E-mail: <a href="mailto:shuohuai@outlook.com">shuohuai [AT] outlook [dot] com</a> <br/>
                            GitHub: <a href="https://github.com/SHUO-HUAI">https://github.com/shuo-huai</a> <br/>
                            Google Scholar: <a href="https://scholar.google.com/citations?user=HWwI12YAAAAJ">https://scholar.google.com/citations?user=HWwI12YAAAAJ</a>
                            <!--<br />Linkedin: <a href="https://www.linkedin.com/in/shuo-huai-7351001a0/">https://www.linkedin.com/in/shuo-huai-7351001a0</a>-->
                        </p>
                        </td>
                    </tr>
                </table>
                <h2>About me</h2>
                <a class="anchor" id="aboutme"></a>
                <p>I received the BSc degree from the School of Computer Science and Technology, Shandong University,
                    Jinan, China, in
                    2019.
                    I am currently pursuing the Ph.D. degree with the School of Computer Science and Engineering,
                    Nanyang Technological University, Singapore.
                    My research interests are  embedded intelligence, neural network optimization, in-memory computing, and non-volatile memory.</p>
                <!--<h2>Research and Publications</h2>-->
                <h3>Research Interests </h3>
                <ul>
                    <li><p>Embedded Intelligence</p>
                    </li>
                    <li><p>Neural Network Optimization</p>
                    </li>
                    <li><p>In-Memory Computing</p>
                    </li>
                    <li><p>Non-Volatile Memory</p>
                    </li>
                </ul>
                <h2>Education</h2>
                <h3>January 2020 — Present &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a
                        href="https://www.ntu.edu.sg/">Nanyang Technological University</a></h3>
                <div style="font-size: 110%"><b>Supervisor: <a href="https://personal.ntu.edu.sg/liu/"> Liu Weichen </a></b>
                </div>
                <div style="font-size: 50%">&nbsp;</div>
                Ph.D. in Computer Science and Engineering

                <h3>September 2015 — June 2019 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a
                        href="https://www.sdu.edu.cn/">Shandong University</a></h3>
                <div style="font-size: 110%"><b>Advisor: <a href="https://zmyhomepage.github.io/zmy_EN/"> Zhao
                    Mengying </a></b>
                </div>
                <div style="font-size: 50%">&nbsp;</div>
                Bachelor of Computer Science and Technology
                <h2>Publications </h2>
                <a class="anchor" id="publications"></a>
                <h3>Conference</h3>
                <ol>
                    <li><p>Chen, Hui; Liu, Di; Li, Shiqing; <b>Huai, Shuo</b>; Luo, Xiangzhong; Liu, Weichen; <a
                            href="https://dl.acm.org/doi/abs/10.1145/3566097.3567846">"MUGNoC: A
                        Software-Configured Multicast-Unicast-Gather NoC for Accelerating CNN Dataflows"</a>,
                        Proceedings of
                        the 28th Asia and South Pacific Design Automation Conference, 308-313, 2023.</p></li>
                    <li><p><b>Huai, Shuo</b>; Liu, Di; Luo, Xiangzhong; Chen, Hui; Liu, Weichen; Subramaniam, Ravi;
                        <a href="https://dl.acm.org/doi/abs/10.1145/3566097.3567856">"Crossbar-Aligned & Integer-Only
                            Neural Network Compression for Efficient in-Memory
                            Acceleration"</a>, Proceedings of the 28th Asia and South Pacific Design Automation
                        Conference,
                        234-239, 2023.</p></li>
                    <li><p>Luo, Xiangzhong; Liu, Di; Kong, Hao; <b>Huai, Shuo</b>; Chen, Hui; Liu, Weichen; <a
                            href="https://dl.acm.org/doi/abs/10.1145/3489517.3530488">"You only
                        search once: on lightweight differentiable architecture search for resource-constrained embedded
                        platforms"</a>, Proceedings of the 59th ACM/IEEE Design Automation Conference, 475-480, 2022.
                    </p>
                    </li>
                    <li><p><b>Huai, Shuo</b>; Liu, Di; Kong, Hao; Luo, Xiangzhong; Liu, Weichen; Subramaniam, Ravi;
                        Makaya, Christian; Lin, Qian; <a href="https://ieeexplore.ieee.org/abstract/document/9978336">"Collate:
                            Collaborative Neural Network Learning for
                            Latency-Critical Edge Systems"</a>, 2022 IEEE 40th International Conference on Computer
                        Design
                        (ICCD), 627-634, 2022, IEEE.</p></li>
                    <li><p>Kong, Hao; Liu, Di; <b>Huai, Shuo</b>; Luo, Xiangzhong; Liu, Weichen; Subramaniam, Ravi;
                        Makaya, Christian; Lin, Qian; <a href="https://dl.acm.org/doi/abs/10.1145/3508352.3549397">"Smart
                            Scissor: Coupling Spatial Redundancy Reduction and CNN
                            Compression for Embedded Hardware"</a>, Proceedings of the 41st IEEE/ACM International
                        Conference on
                        Computer-Aided Design, 9-Jan, 2022.</p></li>
                    <li><p><b>Huai, Shuo</b>; Zhang, Lei; Liu, Di; Liu, Weichen; Subramaniam, Ravi; <a
                            href="https://ieeexplore.ieee.org/abstract/document/9586309">"ZeroBN: Learning
                        Compact Neural Networks For Latency-Critical Edge Systems"</a>, 2021 58th ACM/IEEE Design
                        Automation
                        Conference (DAC),151-156, 2021, IEEE.</p></li>
                    <li><p><b>Huai, Shuo</b>; Song, Weining; Zhao, Mengying; Cai, Xiaojun; Jia, Zhiping; <a
                            href="https://dl.acm.org/doi/abs/10.1145/3316781.3317881">
                        "Performance-aware wear leveling for block ram in nonvolatile fpgas"</a>, Proceedings of the
                        56th
                        Annual Design Automation Conference 2019, pp. 1-6, 2019.</p></li>
                </ol>
                <h3>Journal</h3>
                <ol>
            <li><p><b>Huai, Shuo</b>; Kong, Hao; Li, Shiqing;  Luo, Xiangzhong; Subramaniam, Ravi; Makaya, Christian;
                        Lin, Qian; Liu, Weichen; <a href="https://ieeexplore.ieee.org.remotexs.ntu.edu.sg/document/10269129"> "EvoLP: Self-Evolving Latency Predictor for Model Compression in Real-Time Edge Systems"</a>,
                          IEEE Embedded Systems Letters, 2023, IEEE </p></li>
                        
                       <li><p><b>Huai, Shuo</b>; Kong, Hao; Luo, Xiangzhong; Li, Shiqing; Subramaniam, Ravi; Makaya, Christian;
                        Lin, Qian; Liu, Weichen; <a href="https://dl.acm.org/doi/10.1145/3609115"> "CRIMP:
                               Compact & Reliable DNNs Inference for In-Memory Processing via Crossbar-Aligned Compression and Non-ideality Adaptation"</a>,
                          Transactions on Embedded Computing Systems, 2023, ACM. </p></li>

                    <li><p><b>Huai, Shuo</b>; Kong, Hao; Luo, Xiangzhong;  Liu, Di;  Subramaniam, Ravi; Makaya, Christian;
                        Lin, Qian; Liu, Weichen; <a href="https://ieeexplore.ieee.org/abstract/document/10226296/"> "On
                            Hardware-Aware Design and Optimization of Edge Intelligence"</a>,  IEEE Design & Test, 2023, IEEE. </p></li>
                    <li><p>Luo, Xiangzhong; Liu, Di; Kong, Hao; <b>Huai, Shuo</b>; Chen, Hui; Liu, Weichen; <a
                            href="https://ieeexplore.ieee.org/abstract/document/9817049"> "SurgeNAS: A
                        Comprehensive Surgery on Hardware-Aware Differentiable Neural Architecture Search"</a>, IEEE
                        Transactions on Computers, 2022, IEEE.</p></li>
                    <li><p>Chen, Hui; Chen, Peng; Luo, Xiangzhong; <b>Huai, Shuo</b>; Liu, Weichen; <a
                            href="https://ieeexplore.ieee.org/abstract/document/9709895">"LAMP: Load-bAlanced
                        Multipath Parallel Transmission in Point-to-point NoCs"</a>, IEEE Transactions on Computer-Aided
                        Design of Integrated Circuits and Systems, 2022, IEEE.</p></li>
                    <li><p><b>Huai, Shuo</b>; Liu, Di; Kong, Hao; Liu, Weichen; Subramaniam, Ravi; Makaya, Christian;
                        Lin, Qian; <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167739X22004253">
                            "Latency-constrained DNN architecture learning for edge systems using zerorized batch
                            normalization"</a>, Future Generation Computer Systems, 2022, Elsevier.</p></li>
                    <li><p>Luo, Xiangzhong; Liu, Di; Kong, Hao; <b>Huai, Shuo</b>; Chen, Hui; Liu, Weichen; <a
                            href="https://ieeexplore.ieee.org/abstract/document/9896156">"LightNAS:
                        On Lightweight and Scalable Neural Architecture Search for Embedded Platforms"</a>, IEEE
                        Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2022, IEEE.</p></li>
                    <li><p>Luo, Xiangzhong; Liu, Di; <b>Huai, Shuo</b>; Kong, Hao; Chen, Hui; Liu, Weichen; <a
                            href="https://ieeexplore.ieee.org/document/9496596"> "Designing
                        Efficient DNNs via Hardware-Aware Neural Architecture Search and Beyond"</a>, IEEE Transactions
                        on
                        Computer-Aided Design of Integrated Circuits and Systems, 41, 6, 1799-1812, 2021, IEEE.</p></li>
                    <li><p>Kong, Hao; <b>Huai, Shuo</b>; Liu, Di; Zhang, Lei; Chen, Hui; Zhu, Shien; Li, Shiqing; Liu,
                        Weichen; Rastogi, Manu; Subramaniam, Ravi; <a
                                href="https://ieeexplore.ieee.org/document/9475509">"EDLAB: A benchmark for edge deep
                            learning
                            accelerators"</a>, IEEE Design & Test, 39, 3, 17-Aug, 2021, IEEE.</p></li>
                </ol>
                <p><a href="https://scholar.google.com/citations?user=HWwI12YAAAAJ&hl=en">Full list of publications in
                    Google Scholar.</a></p>
                <h2>Innovations</h2>
                <h3>Patent</h3>
                <ol>
                    <li>
                        <p>Title: A Non-Volatile FPGA Placement Optimization Method and System Based on
                            Performance-aware
                            Wear Leveling<br/>
                            Inventors: 1) Zhao Mengying; 2) <b>Huai Shuo</b>; 3) Shen Zhaoyan; 4) Cai Xiaojun; 5) Jia
                            Zhiping<br/>
                            Filed date: 20 May 2019<br/>
                            Patent No.: ZL 2019 1 0419760.9
                        </p>
                    </li>
                </ol>
                <h3>Technology Disclosure (TD)</h3>
                <ol>
                    <li><p>Title: EDLAB: A Benchmark Tool for Edge Deep Learning Accelerators <br/>
                        Inventors: 1) Liu Weichen; 2) Liu Di; 3) Kong Hao; 4) Zhang Lei; 5) <b>Huai Shuo</b>; 6) Li
                        Shiqing; 7) Chen Hui; 8) Zhu Shien <br/>
                        Filed date: 08 July 2020 <br/>
                        Ref: TD 2020-264 <br/>
                    </p>
                    </li>
                    <li><p>Title: ZeroBN: Learning Compact Neural Networks For Latency-Critical Edge Systems <br/>
                        Inventors: 1) Liu Weichen; 2) <b>Huai Shuo</b>; 3) Liu Di; 4) Zhang Lei <br/>
                        Filed date: 09 March 2021 <br/>
                        Ref: TD 2021-096 <br/>
                    </p>
                    </li>
                    <li><p>Title: Collaborative Neural Network Learning For Multiple Latency-Critical Edge Systems <br/>
                        Inventors: 1) Liu Weichen; 2) <b>Huai Shuo</b>; 3) Kong Hao <br/>
                        Filed date: 15 August 2022 <br/>
                        Ref: TD 2022-287 <br/>
                    </p>
                    </li>
                    <li><p>Title: Smart Scissor: A Deep Compression Framework For Jointly Reducing the Redundancy In
                        Images And Neural Networks <br/>
                        Inventors: 1) Liu Weichen; 2) Kong Hao; 3) <b>Huai Shuo</b> <br/>
                        Filed date: 15 August 2022 <br/>
                        Ref: TD 2022-288 <br/>
                    </p>
                    </li>
                </ol>
                <h2>Projects</h2>
                <a class="anchor" id="projects"></a>
                <ol>
                    <li><p><a href="project/p1.html"> Learning Compact DNNs for Latency-Critical Edge Systems</a></p></li>
                    &nbsp;&nbsp;— Learn compact DNNs under a ‘hard’ latency constraint upon targeted hardware by one
                    training process.
                    <div style="font-size: 50%">&nbsp;</div>
                    <ul>
                        <li><p>We propose a novel compact model training method ZeroBN that
                            can extract the optimal architecture to satisfy a ‘hard’ latency
                            constraint by only one training process;</p>
                        </li>
                        <li><p>We propose a scheme applying the global importance rank of
                            channels and constructing a dynamic Zero-Recovery training
                            process. It extends the exploration space of ZeroBN to learn
                            the optimal architecture.</p>
                        </li>
                        <li><p>We propose a machine learning-based latency predictor, which
                            is embedded into ZeroBN to provide a compression ratio for
                            each Zero phase.</p>
                        </li>
                        <li><p>We demonstrate the efficiency and effectiveness of our ZeroBN
                            with extensive experiments. Without the need to perform refinement, our method does not
                            cause a large accuracy drop
                            compared to the original model. On CIFAR-10 dataset, we
                            even improve the accuracy of all models by 0.24% to 0.32%.
                            On ImageNet-100 dataset, under a latency constraint, we
                            improve 0.3% accuracy for GoogLeNet.</p></li>
                    </ul>
                    <li><p><a href="project/p2.html">Collaborative DNNs Learning for Multiple Latency-Critical Edge Systems </a>
                    </p></li>
                    &nbsp;&nbsp;— Optimize federated learning algorithm to simultaneously meet the latency constraints
                    of all participating systems while obtaining high accuracy.
                    <div style="font-size: 50%">&nbsp;</div>
                    <ul>
                        <li><p>We propose a novel model learning framework, Collate, that
                            cultivates optimal DNN architectures collaboratively for multiple
                            edge systems to obtain higher accuracy and satisfy their latency
                            constraints with only one training process.</p>
                        </li>
                        <li><p>We present a proto-corrected aggregation scheme in the global
                            training process to effectively aggregate all heterogeneous models from each edge system for
                            higher accuracy.</p>
                        </li>
                        <li><p>We demonstrate the effectiveness of Collate with extensive
                            experiments. Compared to the state-of-the-art methods and under
                            the same latency constraints, our extended models can improve
                            the accuracy by 1.96% on average, and the accuracy of shrunk
                            models outperforms others by 3.09% on average.</p></li>
                    </ul>
                    <li><p><a href="project/p3.html">In-Memory Computing Based Neural Network Compression</a></p></li>
                    &nbsp;&nbsp;— Apply crossbar-aligned pruning and integer-only quantization to completely achieve
                    efficient and low-power IMC acceleration.
                    <div style="font-size: 50%">&nbsp;</div>
                    <ul>
                        <li><p>We introduce a crossbar-aligned pruning approach to reduce crossbar usage without extra
                            processing units for better hardware efficiency and greater integration density. Also, it
                            includes both kernel-group pruning and crossbar pruning to form multi-grained pruning for
                            high accuracy and large sparsity. </p>
                        </li>
                        <li><p>We apply a simple yet efficient integer-only quantization scheme for IMC architecture by
                            reusing the bit-shift units. The quantization approach is co-optimized with the pruning
                            strategy during the training process to improve accuracy.</p>
                        </li>
                        <li><p>We demonstrate the efficiency and effectiveness of our framework with extensive
                            experiments. Compared to state-of-the-art methods, our method can achieve a higher sparsity
                            rate and a slighter accuracy drop without extra hardware. Thus, we can reduce computing
                            power and computing area significantly.</p>
                        </li>
                    </ul>
                    <li><p><a href="project/p4.html">Performance-aware Wear Leveling Placement Algorithm for Nonvolatile
                        FPGAs</a></p></li>
                    &nbsp;&nbsp;— A performance-aware wear leveling
                    strategy for BRAM blocks in non-volatile FPGAs to improve the
                    lifetime, while maintaining high efficiency.
                    <div style="font-size: 50%">&nbsp;</div>
                    <ul>
                        <li><p>Propose to simultaneously consider lifetime and performance in WL for BRAM blocks in FPGA
                            systems.</p>
                        </li>
                        <li><p>Design a performance-aware WL mechanism to balance
                            write traffic between BRAM blocks. The placement strategy is improved by integrating write
                            information so that
                            multiple configuration files can be generated for dynamic
                            switching.</p>
                        </li>
                        <li><p>Develop a simulator to evaluate performance and lifetime
                            for non-volatile FPGAs. The evaluation results show significant lifetime improvement with
                            little performance overhead.</p>
                        </li>
                    </ul>

                </ol>


                <h2>Experience</h2>
                <a class="anchor" id="experience"></a>
                <h3>Work</h3>
                <ol>
                    <li><p>Project Officer, <a href="https://www.ntu.edu.sg/hp-ntu-corp-lab">HP-NTU Digital
                        Manufacturing Corporate Lab</a>, September 2019 - October 2023</p></li>
                    &nbsp;&nbsp;— Machine Learning Optimization for Edge Devices
                    <ul>
                        <li><p>Design a performance evaluation methodology and toolset for Machine Learning Accelerators
                            (MLAs).</p>
                        </li>
                        <li><p> Provide HP business groups, like Business Personal Systems and Digital Manufacturing,
                            the insights and framework for process and resource efficient design and optimizing of DNN,
                            accelerators and system architectures for machine learning/inference at the edge.</p></li>
                    </ul>
                </ol>

                <h3>Teaching</h3>
                <ol>
                    <li><p>Teaching Assistant, <a
                        href="https://www.ntu.edu.sg/">Nanyang Technological University</a>, February 2020 - March 2020 </p></li>
                    <ul>
                        <li><p>CE/CZ 1006 Computer Organisation & Architecture</p>
                        </li>
                    </ul>
                        <li><p>Teaching Assistant, <a
                        href="https://www.sdu.edu.cn/">Shandong University</a>, February 2019 - June 2019 </p></li>
                    <ul>
                        <li><p>SD01331470 Computer Organization and Design</p>
                        </li>
                    </ul>
                </ol>
                <div style="font-size:100%">&nbsp;</div>
                <div>
                    <hr/>
                </div>
                <div align="center" style=" width: 100%" ><img
                        src="https://s11.flagcounter.com/count/Bj2R/bg_FFFFFF/txt_000000/border_ffffff/columns_8/maxflags_8/viewers_3/labels_1/pageviews_0/flags_0/percent_0"
                        alt="Flag Counter"></div>
                <div style="font-size:50%">&nbsp;</div>
                <div style="height: 38px; width: 0.1%;  background-color: rgba(198,198,198,0.6); text-align: center; font-size: 90%; display: table-cell; vertical-align: middle; table-layout: auto;  ">
                    Copyright &copy; 2022-
                    <script>document.write(new Date().getFullYear())</script>
                    Huai Shuo      |               <SCRIPT LANGUAGE="JavaScript">
                        monthNames = ["January", "February", "March", "April",
                            "May", "June", "July", "August",
                            "September", "October", "November", "December"];
                        update = new Date(document.lastModified)
                        theMonth = update.getMonth()
                        theDate = update.getDate()
                        theYear = update.getFullYear()
                        monthName = monthNames[theMonth];
                        document.writeln("Last updated: " + theDate + " " + monthName + " " + theYear)
                    </SCRIPT>
                </div>
            </div>
        </td>
    </tr>
</table>
</body>
</html>
